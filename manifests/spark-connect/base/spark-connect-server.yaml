# Spark Connect Server - Long-running Spark service with gRPC endpoint
#
# Provides a persistent Spark session that clients can connect to via Spark Connect.
# Clients use: SparkSession.builder.remote("sc://spark-connect.spark.svc:15002").getOrCreate()
#
# Deploy:  kubectl apply -k ops/app-spark-connect/base/
# Check:   kubectl get pods -n spark -l app=spark-connect-server
# Logs:    kubectl logs -n spark -l app=spark-connect-server
# Delete:  kubectl delete -k ops/app-spark-connect/base/
#
# Connect from magnum:
#   export SPARK_REMOTE="sc://spark-connect.spark.svc:15002"
#   pyspark
#
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-connect-server
  namespace: spark
  labels:
    app: spark-connect-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-connect-server
  template:
    metadata:
      labels:
        app: spark-connect-server
    spec:
      serviceAccountName: spark-operator-spark
      nodeSelector:
        kubernetes.io/hostname: cyberpower
      containers:
        - name: spark-connect
          image: localhost:32000/electinfo/spark-connect-server:latest
          imagePullPolicy: Always
          command:
            - /bin/bash
            - -c
            - |
              # Create Ivy cache directory (spark user home is /nonexistent in /etc/passwd)
              mkdir -p /nonexistent/.ivy2.5.2/cache /nonexistent/.ivy2.5.2/jars

              # Start Spark Connect server with S3A support for asustor
              exec /opt/spark/sbin/start-connect-server.sh \
                --master k8s://https://kubernetes.default.svc:443 \
                --deploy-mode client \
                --name spark-connect-server \
                --conf spark.ui.prometheus.enabled=true \
                --conf spark.metrics.conf=/opt/spark/conf/metrics.properties \
                --conf spark.jars.packages=org.apache.hadoop:hadoop-aws:3.4.2,software.amazon.awssdk:bundle:2.29.6,org.neo4j:neo4j-connector-apache-spark_2.13:5.3.2_for_spark_3,org.postgresql:postgresql:42.7.1,io.graphframes:graphframes-spark4_2.13:0.10.0 \
                --conf spark.jars.ivy=/nonexistent/.ivy2.5.2 \
                --conf spark.driver.host=$POD_IP \
                --conf spark.kubernetes.container.image=localhost:32000/electinfo/spark-executor:latest \
                --conf spark.kubernetes.namespace=spark \
                --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-operator-spark \
                --conf spark.executor.instances=5 \
                --conf spark.executor.cores=8 \
                --conf spark.executor.memory=24g \
                --conf spark.driver.memory=8g \
                --conf spark.hadoop.hive.metastore.uris=thrift://app-hive.electinfo.svc.cluster.local:9083 \
                --conf spark.sql.catalogImplementation=hive \
                --conf spark.sql.hive.metastore.version=4.1.0 \
                --conf spark.sql.hive.metastore.jars=maven \
                --conf spark.sql.warehouse.dir=s3a://electinfo/hive-warehouse \
                --conf spark.eventLog.enabled=true \
                --conf spark.eventLog.dir=s3a://spark-history/ \
                --conf spark.dynamicAllocation.enabled=false \
                --conf spark.dynamicAllocation.minExecutors=1 \
                --conf spark.dynamicAllocation.maxExecutors=1 \
                --conf spark.dynamicAllocation.executorIdleTimeout=120s \
                --conf spark.kubernetes.executor.podTemplateFile=/opt/spark/conf/executor-pod-template.yaml \
                --conf spark.kubernetes.file.upload.path=/tmp/spark-uploads \
                --conf spark.hadoop.fs.s3a.buffer.dir=/tmp/spark-local \
                --conf spark.hadoop.fs.s3a.endpoint=http://10.10.0.10:9000 \
                --conf spark.hadoop.fs.s3a.path.style.access=true \
                --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
                --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
                --conf spark.hadoop.fs.s3a.access.key=electinfo \
                --conf spark.hadoop.fs.s3a.secret.key=electinfo123 \
                --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider \
                --conf spark.hadoop.fs.s3a.change.detection.mode=none
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: SPARK_NO_DAEMONIZE
              value: "true"
          ports:
            - name: spark-connect
              containerPort: 15002
              protocol: TCP
            - name: spark-ui
              containerPort: 4040
              protocol: TCP
            - name: driver-rpc
              containerPort: 7078
              protocol: TCP
            - name: block-manager
              containerPort: 7079
              protocol: TCP
          resources:
            requests:
              memory: "8Gi"
              cpu: "4"
            limits:
              memory: "16Gi"
              cpu: "8"
          volumeMounts:
            - name: spark-conf
              mountPath: /opt/spark/conf/executor-pod-template.yaml
              subPath: executor-pod-template.yaml
            - name: spark-conf
              mountPath: /opt/spark/conf/metrics.properties
              subPath: metrics.properties
            - name: ivy-cache
              mountPath: /nonexistent
            - name: spark-local
              mountPath: /tmp/spark-local
      volumes:
        - name: spark-conf
          configMap:
            name: spark-connect-config
        - name: ivy-cache
          emptyDir: {}
        - name: spark-local
          emptyDir: {}

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-connect-config
  namespace: spark
data:
  metrics.properties: |
    # Enable Prometheus metrics export
    *.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet
    *.sink.prometheusServlet.path=/metrics/prometheus
    master.source.jvm.class=org.apache.spark.metrics.source.JvmSource
    worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource
    driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource
    executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource
  core-site.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>fs.s3a.endpoint</name>
        <value>http://10.10.0.10:9000</value>
      </property>
      <property>
        <name>fs.s3a.path.style.access</name>
        <value>true</value>
      </property>
      <property>
        <name>fs.s3a.impl</name>
        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
      </property>
      <property>
        <name>fs.s3a.connection.ssl.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>fs.s3a.access.key</name>
        <value>electinfo</value>
      </property>
      <property>
        <name>fs.s3a.secret.key</name>
        <value>electinfo123</value>
      </property>
      <property>
        <name>fs.s3a.aws.credentials.provider</name>
        <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</value>
      </property>
      <property>
        <name>fs.s3a.change.detection.mode</name>
        <value>none</value>
      </property>
      <property>
        <name>fs.s3a.buffer.dir</name>
        <value>/tmp/spark-local</value>
      </property>
    </configuration>
  executor-pod-template.yaml: |
    apiVersion: v1
    kind: Pod
    spec:
      terminationGracePeriodSeconds: 30
      containers:
        - name: spark-kubernetes-executor
          volumeMounts:
            - name: spark-local
              mountPath: /tmp/spark-local
            - name: spark-checkpoints
              mountPath: /mnt/spark-checkpoints
      volumes:
        - name: spark-local
          emptyDir:
            sizeLimit: 20Gi
        - name: spark-checkpoints
          nfs:
            server: 10.10.0.10
            path: /volume1/spark-checkpoints
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  spark-role: executor
              topologyKey: kubernetes.io/hostname

---
# Service to expose Spark Connect endpoint
apiVersion: v1
kind: Service
metadata:
  name: spark-connect
  namespace: spark
  labels:
    app: spark-connect-server
spec:
  type: NodePort
  selector:
    app: spark-connect-server
  ports:
    - name: spark-connect
      port: 15002
      targetPort: 15002
      nodePort: 31500
      protocol: TCP
    - name: spark-ui
      port: 4040
      targetPort: 4040
      nodePort: 30404
      protocol: TCP