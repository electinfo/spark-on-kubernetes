apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-operator-config
  namespace: spark
data:
  spark-defaults.conf: |
    # Hive Metastore
    spark.hadoop.hive.metastore.uris=thrift://app-hive.electinfo.svc.cluster.local:9083
    spark.sql.catalogImplementation=hive
    spark.sql.hive.metastore.version=4.1.0
    spark.sql.hive.metastore.jars=path
    spark.sql.hive.metastore.jars.path=file:///opt/spark/hive4/*
    spark.sql.warehouse.dir=s3a://hive-warehouse/

    # S3A Configuration (s3proxy on asustor)
    spark.hadoop.fs.defaultFS=s3a://static-pages
    spark.hadoop.fs.s3a.endpoint=http://10.10.0.10:9000
    spark.hadoop.fs.s3a.path.style.access=true
    spark.hadoop.fs.s3a.access.key=electinfo
    spark.hadoop.fs.s3a.secret.key=electinfo123

    # Ivy Cache (writable)
    spark.jars.ivySettings=/opt/spark/conf/ivysettings.xml

    # Performance
    spark.eventLog.enabled=false
    spark.sql.adaptive.enabled=true

    # Ivy and home directory settings (log4j config is set via JAVA_TOOL_OPTIONS env var in SparkApplication)
    spark.driver.extraJavaOptions=-Duser.home=/tmp -Divy.default.ivy.user.dir=/tmp/.ivy2 -Divy.settings.file=/opt/spark/conf/ivysettings.xml
    spark.executor.extraJavaOptions=-Duser.home=/tmp -Divy.default.ivy.user.dir=/tmp/.ivy2

  # NOTE: Do NOT add log4j2.properties here. The Spark Operator v2.1.0 webhook
  # auto-detects this key and injects a subPath volume mount at
  # /opt/spark/conf/log4j2.properties, which fails because /opt/spark/conf
  # already has projected volumes. Logging is controlled via
  # -Droot.logger.level=WARN in extraJavaOptions instead.